{
 "cells": [
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "def split_data(source_dir, train_dir, val_dir, split_ratio=(0.9, 0.1)):\n",
    "    # Create directories if they don't exist\n",
    "    for directory in [train_dir, val_dir]:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "\n",
    "    # Get list of all files in the source directory\n",
    "    files = os.listdir(source_dir)\n",
    "    # Shuffle the files\n",
    "    random.shuffle(files)\n",
    "\n",
    "    # Calculate split indices\n",
    "    total_files = len(files)\n",
    "    train_split = int(total_files * split_ratio[0])\n",
    "\n",
    "    # Split files into train, val, and test sets\n",
    "    train_files = files[:train_split]\n",
    "    val_files = files[train_split:]\n",
    "\n",
    "    # Copy files to respective directories\n",
    "    for file in train_files:\n",
    "        shutil.copy(os.path.join(source_dir, file), os.path.join(train_dir, file))\n",
    "    for file in val_files:\n",
    "        shutil.copy(os.path.join(source_dir, file), os.path.join(val_dir, file))\n",
    "\n",
    "# Example usage\n",
    "source_directory = r'D:\\code_study\\ML_CODE\\dataSets\\ObjectDetection\\Human_Faces_Object_Detection\\archive\\images'\n",
    "source_directory = source_directory.replace('\\\\', '/')\n",
    "train_directory = './images/train'\n",
    "val_directory = './images/val'\n",
    "\n",
    "split_data(source_directory, train_directory, val_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(r'D:\\code_study\\ML_CODE\\dataSets\\ObjectDetection\\Human_Faces_Object_Detection\\archive\\faces.csv')\n",
    "df['class'] = 'human'\n",
    "df.columns = ['filename','width','height','xmin','ymin','xmax','ymax','class']\n",
    "df = df[['filename','class','width', 'height','xmin','ymin','xmax','ymax']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dir = './labels'\n",
    "\n",
    "# Create the labels directory if it doesn't exist\n",
    "if not os.path.exists(labels_dir):\n",
    "    os.makedirs(labels_dir)\n",
    "\n",
    "# Subdirectories within labels directory\n",
    "train_dir = os.path.join(labels_dir, 'train')\n",
    "val_dir = os.path.join(labels_dir, 'val')\n",
    "\n",
    "# Create train, val, and test directories within labels directory\n",
    "for directory in [train_dir, val_dir]:\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = './images/train'\n",
    "val_dir = './images/val'\n",
    "\n",
    "# Function to get filenames in a directory\n",
    "def get_filenames_in_dir(directory):\n",
    "    return set(os.listdir(directory))\n",
    "\n",
    "# Get filenames in each directory\n",
    "train_filenames = get_filenames_in_dir(train_dir)\n",
    "val_filenames = get_filenames_in_dir(val_dir)\n",
    "\n",
    "# Filter DataFrame based on filenames\n",
    "train_df = df[df['filename'].isin(train_filenames)]\n",
    "val_df = df[df['filename'].isin(val_filenames)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO format .txt files created successfully.\n",
      "YOLO format .txt files created successfully.\n"
     ]
    }
   ],
   "source": [
    "def convert_to_yolo(df, output_dir):\n",
    "    # Create the output directory if it doesn't exist\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Function to convert bounding box coordinates to YOLO format\n",
    "    def convert_to_yolo_format(width, height, xmin, ymin, xmax, ymax):\n",
    "        x_center = (xmin + xmax) / (2 * width)\n",
    "        y_center = (ymin + ymax) / (2 * height)\n",
    "        box_width = (xmax - xmin) / width\n",
    "        box_height = (ymax - ymin) / height\n",
    "        return x_center, y_center, box_width, box_height\n",
    "\n",
    "    # Loop over the DataFrame and create YOLO format .txt files\n",
    "    for index, row in df.iterrows():\n",
    "        filename = row['filename']\n",
    "        width = row['width']\n",
    "        height = row['height']\n",
    "        xmin = row['xmin']\n",
    "        ymin = row['ymin']\n",
    "        xmax = row['xmax']\n",
    "        ymax = row['ymax']\n",
    "\n",
    "        x_center, y_center, box_width, box_height = convert_to_yolo_format(width, height, xmin, ymin, xmax, ymax)\n",
    "\n",
    "        # Write YOLO format data to a .txt file\n",
    "        output_file = os.path.join(output_dir, os.path.splitext(filename)[0] + \".txt\")\n",
    "        with open(output_file, 'w') as f:\n",
    "            f.write(f\"0 {x_center} {y_center} {box_width} {box_height}\\n\")\n",
    "\n",
    "    print(\"YOLO format .txt files created successfully.\")\n",
    "\n",
    "\n",
    "convert_to_yolo(val_df, './labels/val')\n",
    "convert_to_yolo(train_df, './labels/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YAML file 'data.yaml' created successfully.\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "data = {\n",
    "    'train': './images/train',\n",
    "    'val': './images/val',\n",
    "    'nc': 1,\n",
    "    'names': {\n",
    "        '0': 'human'\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('data.yaml', 'w') as file:\n",
    "    yaml.dump(data, file, default_flow_style=False)\n",
    "\n",
    "print(\"YAML file 'data.yaml' created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.65  Python-3.9.21 torch-2.5.1+cu118 CUDA:0 (NVIDIA GeForce GTX 1660 SUPER, 6144MiB)\n",
      "Setup complete  (20 CPUs, 31.8 GB RAM, 920.5/931.5 GB disk)\n"
     ]
    }
   ],
   "source": [
    "from IPython import display\n",
    "display.clear_output()\n",
    "\n",
    "import ultralytics\n",
    "ultralytics.checks()\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "from IPython.display import display, Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.65  Python-3.9.21 torch-2.5.1+cu118 CUDA:0 (NVIDIA GeForce GTX 1660 SUPER, 6144MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=D:\\code_study\\ML_CODE\\kaggle\\ObjectDetection\\Human Faces Object Detection\\Face Detection using YOLO\\data.yaml, epochs=30, time=None, patience=5, batch=16, imgsz=320, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train6, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train6\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "Model summary: 225 layers, 3,011,043 parameters, 3,011,027 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks failed . AMP training on NVIDIA GeForce GTX 1660 SUPER GPU may cause NaN losses or zero-mAP results, so AMP will be disabled during training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\code_study\\ML_CODE\\kaggle\\ObjectDetection\\Human Faces Object Detection\\Face Detection using YOLO\\labels\\train... 2202 images, 0 backgrounds, 0 corrupt: 100%|██████████| 2202/2202 [00:02<00:00, 747.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: D:\\code_study\\ML_CODE\\kaggle\\ObjectDetection\\Human Faces Object Detection\\Face Detection using YOLO\\labels\\train.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\code_study\\ML_CODE\\kaggle\\ObjectDetection\\Human Faces Object Detection\\Face Detection using YOLO\\labels\\val... 593 images, 0 backgrounds, 0 corrupt: 100%|██████████| 593/593 [00:00<00:00, 821.33it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: D:\\code_study\\ML_CODE\\kaggle\\ObjectDetection\\Human Faces Object Detection\\Face Detection using YOLO\\labels\\val.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\train6\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 320 train, 320 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train6\u001b[0m\n",
      "Starting training for 30 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/30      1.07G      1.759      2.079       1.55         27        320: 100%|██████████| 138/138 [00:12<00:00, 10.93it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:02<00:00,  8.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        593        593      0.415      0.681      0.483       0.22\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/30      1.12G      1.577      1.588      1.422         16        320: 100%|██████████| 138/138 [00:11<00:00, 12.40it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:01<00:00,  9.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        593        593      0.562      0.695      0.652      0.356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/30      1.08G      1.571      1.468      1.424         21        320: 100%|██████████| 138/138 [00:10<00:00, 12.57it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:01<00:00,  9.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        593        593      0.694       0.71      0.759      0.411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/30      1.12G      1.522      1.385      1.417         20        320: 100%|██████████| 138/138 [00:11<00:00, 12.44it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:01<00:00,  9.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        593        593      0.813      0.666      0.807      0.484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/30      1.08G      1.479      1.335      1.397         24        320: 100%|██████████| 138/138 [00:10<00:00, 12.62it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:01<00:00,  9.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        593        593      0.746      0.659       0.77      0.458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/30      1.12G      1.439      1.268      1.371         18        320: 100%|██████████| 138/138 [00:10<00:00, 12.57it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:01<00:00,  9.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        593        593      0.811      0.761      0.862       0.53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/30      1.08G      1.404      1.233      1.353         22        320: 100%|██████████| 138/138 [00:10<00:00, 12.72it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:01<00:00,  9.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        593        593      0.773      0.747       0.83      0.518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/30      1.12G      1.382      1.196      1.337         19        320: 100%|██████████| 138/138 [00:11<00:00, 12.53it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:01<00:00,  9.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        593        593      0.728      0.749      0.818      0.495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/30      1.08G      1.347      1.158      1.311         17        320: 100%|██████████| 138/138 [00:10<00:00, 12.61it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:01<00:00,  9.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        593        593      0.868      0.745      0.888      0.581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/30      1.12G      1.332      1.126      1.311         23        320: 100%|██████████| 138/138 [00:10<00:00, 12.65it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:01<00:00,  9.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        593        593       0.78      0.757      0.843      0.525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      11/30      1.08G       1.28      1.075      1.287         20        320: 100%|██████████| 138/138 [00:10<00:00, 12.61it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:01<00:00,  9.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        593        593      0.873      0.728      0.895      0.591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      12/30      1.12G      1.268      1.067      1.266         18        320: 100%|██████████| 138/138 [00:10<00:00, 12.62it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:01<00:00,  9.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        593        593      0.841      0.777      0.894      0.613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      13/30      1.08G      1.263      1.056      1.275         19        320: 100%|██████████| 138/138 [00:10<00:00, 12.76it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:01<00:00,  9.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        593        593       0.84      0.804       0.88      0.605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      14/30      1.12G      1.228      1.019      1.253         22        320: 100%|██████████| 138/138 [00:10<00:00, 12.73it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:01<00:00,  9.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        593        593       0.91      0.753      0.909      0.632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      15/30      1.08G      1.226      1.008      1.249         22        320: 100%|██████████| 138/138 [00:10<00:00, 12.74it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:01<00:00,  9.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        593        593      0.869      0.793      0.916      0.638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      16/30      1.12G      1.196     0.9928      1.235         24        320: 100%|██████████| 138/138 [00:10<00:00, 12.72it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:01<00:00,  9.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        593        593      0.874      0.766      0.906      0.646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      17/30      1.08G      1.188     0.9918      1.235         21        320: 100%|██████████| 138/138 [00:10<00:00, 12.65it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:01<00:00, 10.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        593        593      0.879      0.799      0.916      0.656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      18/30      1.12G      1.171     0.9525      1.221         24        320: 100%|██████████| 138/138 [00:10<00:00, 12.59it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:01<00:00,  9.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        593        593      0.886        0.8      0.925      0.666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      19/30      1.08G      1.171     0.9398      1.214         18        320: 100%|██████████| 138/138 [00:10<00:00, 12.69it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:01<00:00, 10.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        593        593      0.908      0.791      0.926      0.672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      20/30      1.12G      1.153     0.9284      1.218         17        320: 100%|██████████| 138/138 [00:10<00:00, 12.72it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:01<00:00, 10.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        593        593      0.933      0.792      0.932      0.684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      21/30      1.08G      1.042     0.7785      1.184         10        320: 100%|██████████| 138/138 [00:11<00:00, 12.52it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:01<00:00,  9.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        593        593      0.915      0.798      0.933      0.665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      22/30      1.12G      1.012     0.7214      1.171         10        320: 100%|██████████| 138/138 [00:10<00:00, 12.79it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:01<00:00, 10.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        593        593      0.933      0.794      0.936      0.668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      23/30      1.08G      1.001     0.6912      1.161         10        320: 100%|██████████| 138/138 [00:10<00:00, 12.75it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:01<00:00, 10.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        593        593       0.91        0.8      0.936      0.677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      24/30      1.12G     0.9642     0.6841      1.144         10        320: 100%|██████████| 138/138 [00:10<00:00, 12.78it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:01<00:00, 10.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        593        593      0.893      0.806      0.933      0.656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      25/30      1.08G     0.9524      0.669      1.124         10        320: 100%|██████████| 138/138 [00:10<00:00, 12.80it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:01<00:00, 10.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        593        593      0.917      0.818      0.948        0.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      26/30      1.12G     0.9216     0.6581      1.105         10        320: 100%|██████████| 138/138 [00:10<00:00, 12.80it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:01<00:00, 10.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        593        593      0.945      0.809      0.952      0.713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      27/30      1.08G       0.91      0.633      1.109         10        320: 100%|██████████| 138/138 [00:10<00:00, 12.79it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:01<00:00, 10.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        593        593       0.96      0.802      0.955      0.725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      28/30      1.12G     0.8886     0.6285      1.089         10        320: 100%|██████████| 138/138 [00:10<00:00, 12.78it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:01<00:00, 10.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        593        593      0.943      0.804      0.953      0.703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      29/30      1.08G     0.8785     0.6172      1.084         10        320: 100%|██████████| 138/138 [00:10<00:00, 12.71it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:01<00:00, 10.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        593        593      0.919       0.84      0.957      0.713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      30/30      1.12G     0.8505     0.5999      1.071         10        320: 100%|██████████| 138/138 [00:10<00:00, 12.72it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:01<00:00,  9.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        593        593      0.913       0.85      0.957      0.714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "30 epochs completed in 0.115 hours.\n",
      "Optimizer stripped from runs\\detect\\train6\\weights\\last.pt, 6.2MB\n",
      "Optimizer stripped from runs\\detect\\train6\\weights\\best.pt, 6.2MB\n",
      "\n",
      "Validating runs\\detect\\train6\\weights\\best.pt...\n",
      "Ultralytics 8.3.65  Python-3.9.21 torch-2.5.1+cu118 CUDA:0 (NVIDIA GeForce GTX 1660 SUPER, 6144MiB)\n",
      "Model summary (fused): 168 layers, 3,005,843 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 19/19 [00:02<00:00,  8.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        593        593       0.96      0.802      0.955      0.726\n",
      "Speed: 0.1ms preprocess, 0.8ms inference, 0.0ms loss, 0.7ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train6\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model = YOLO('yolov8n.pt')\n",
    "\n",
    "results = model.train(data='D:\\code_study\\ML_CODE\\kaggle\\ObjectDetection\\Human Faces Object Detection\\Face Detection using YOLO\\data.yaml', epochs=30,imgsz=320,batch=16, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'runs\\\\detect\\\\train\\\\weights\\\\best.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpatches\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpatches\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m testmodel \u001b[38;5;241m=\u001b[39m \u001b[43mYOLO\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./runs/detect/train/weights/best.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdrawbox\u001b[39m(image_path):\n\u001b[0;32m      8\u001b[0m     img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(image_path)\n",
      "File \u001b[1;32mc:\\Users\\Leaper\\anaconda3\\envs\\yoloV5\\lib\\site-packages\\ultralytics\\models\\yolo\\model.py:23\u001b[0m, in \u001b[0;36mYOLO.__init__\u001b[1;34m(self, model, task, verbose)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m \u001b[38;5;241m=\u001b[39m new_instance\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# Continue with default YOLO initialization\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Leaper\\anaconda3\\envs\\yoloV5\\lib\\site-packages\\ultralytics\\engine\\model.py:146\u001b[0m, in \u001b[0;36mModel.__init__\u001b[1;34m(self, model, task, verbose)\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(model, task\u001b[38;5;241m=\u001b[39mtask, verbose\u001b[38;5;241m=\u001b[39mverbose)\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 146\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;66;03m# Delete super().training for accessing self.model.training\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining\n",
      "File \u001b[1;32mc:\\Users\\Leaper\\anaconda3\\envs\\yoloV5\\lib\\site-packages\\ultralytics\\engine\\model.py:289\u001b[0m, in \u001b[0;36mModel._load\u001b[1;34m(self, weights, task)\u001b[0m\n\u001b[0;32m    286\u001b[0m weights \u001b[38;5;241m=\u001b[39m checks\u001b[38;5;241m.\u001b[39mcheck_model_file_from_stem(weights)  \u001b[38;5;66;03m# add suffix, i.e. yolo11n -> yolo11n.pt\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m Path(weights)\u001b[38;5;241m.\u001b[39msuffix \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 289\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mckpt \u001b[38;5;241m=\u001b[39m \u001b[43mattempt_load_one_weight\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    290\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moverrides \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset_ckpt_args(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39margs)\n",
      "File \u001b[1;32mc:\\Users\\Leaper\\anaconda3\\envs\\yoloV5\\lib\\site-packages\\ultralytics\\nn\\tasks.py:908\u001b[0m, in \u001b[0;36mattempt_load_one_weight\u001b[1;34m(weight, device, inplace, fuse)\u001b[0m\n\u001b[0;32m    906\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mattempt_load_one_weight\u001b[39m(weight, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, fuse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    907\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Loads a single model weights.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 908\u001b[0m     ckpt, weight \u001b[38;5;241m=\u001b[39m \u001b[43mtorch_safe_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# load ckpt\u001b[39;00m\n\u001b[0;32m    909\u001b[0m     args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mDEFAULT_CFG_DICT, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(ckpt\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_args\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))}  \u001b[38;5;66;03m# combine model and default args, preferring model args\u001b[39;00m\n\u001b[0;32m    910\u001b[0m     model \u001b[38;5;241m=\u001b[39m (ckpt\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mema\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m ckpt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat()  \u001b[38;5;66;03m# FP32 model\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Leaper\\anaconda3\\envs\\yoloV5\\lib\\site-packages\\ultralytics\\nn\\tasks.py:835\u001b[0m, in \u001b[0;36mtorch_safe_load\u001b[1;34m(weight, safe_only)\u001b[0m\n\u001b[0;32m    833\u001b[0m                 ckpt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(f, pickle_module\u001b[38;5;241m=\u001b[39msafe_pickle)\n\u001b[0;32m    834\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 835\u001b[0m             ckpt \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    837\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# e.name is missing module name\u001b[39;00m\n\u001b[0;32m    838\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Leaper\\anaconda3\\envs\\yoloV5\\lib\\site-packages\\ultralytics\\utils\\patches.py:86\u001b[0m, in \u001b[0;36mtorch_load\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TORCH_1_13 \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights_only\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[0;32m     84\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights_only\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _torch_load(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Leaper\\anaconda3\\envs\\yoloV5\\lib\\site-packages\\torch\\serialization.py:1319\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m   1317\u001b[0m     pickle_load_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1319\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m   1321\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m   1322\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m   1323\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m   1324\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32mc:\\Users\\Leaper\\anaconda3\\envs\\yoloV5\\lib\\site-packages\\torch\\serialization.py:659\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    657\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    658\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 659\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    660\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    661\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32mc:\\Users\\Leaper\\anaconda3\\envs\\yoloV5\\lib\\site-packages\\torch\\serialization.py:640\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    639\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 640\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'runs\\\\detect\\\\train\\\\weights\\\\best.pt'"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "testmodel = YOLO(\"./runs/detect/train/weights/best.pt\")\n",
    "\n",
    "def drawbox(image_path):\n",
    "    img = Image.open(image_path)\n",
    "    results = testmodel.predict(img)\n",
    "\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(img)  # Display the image\n",
    "\n",
    "    # Loop through all detections\n",
    "    for r in results:\n",
    "        # Assuming r.boxes contains all boxes for this particular result, iterate through them\n",
    "        for box in r.boxes.data.tolist():\n",
    "            xmin, ymin, xmax, ymax, conf, cls_id = box\n",
    "            conf = round(conf, 2)\n",
    "            # Create a Rectangle patch\n",
    "            rect = patches.Rectangle((xmin, ymin), xmax-xmin, ymax-ymin, linewidth=1, edgecolor='r', facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "            # Annotate the image with confidence\n",
    "            ax.text(xmin, ymin, f'Conf: {conf}', bbox=dict(facecolor='white', alpha=0.5))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drawbox('/kaggle/input/human-faces-object-detection/images/00000031.jpg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yoloV5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
